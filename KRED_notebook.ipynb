{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XenmVZLO1tAQ"
      },
      "source": [
        "# **Deep NLP project - Recommendation Systems**\n",
        "\n",
        "Python Notebook for the development and testing of the team project for the *Deep Natural Language Processing* course (a.a. 2022/2023) @ *Politecnico di Torino*.\n",
        "\n",
        "The [corresponding GitHub repository](https://github.com/micolrosini/KRED-Reccomendation-System) consists of the following contents:\n",
        "- Implementation of the code\n",
        "- Problem statement & related works\n",
        "- Dataset description\n",
        "- Report paper about the whole activity\n",
        "- Link to this notebook for the quick execution of the code\n",
        "\n",
        "***Authors:*** \\\\\n",
        "*@micolrosini - s302935@studenti.polito.it* \\\\\n",
        "*@gaiasabbatini - s291532@studenti.polito.it* \\\\\n",
        "*@matteogarbarino - s265386@studenti.polito.it*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB6KyeqgJRs0"
      },
      "source": [
        "## 1. Import libraries & install the needed packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr6X0AgQ2LoW"
      },
      "outputs": [],
      "source": [
        "# Import libraries & install packages\n",
        "from google.colab import drive      # to mount a Drive containing the dataset\n",
        "import os, sys, importlib           # to handle files and directories\n",
        "from time import time               # to estimate performances\n",
        "import tarfile                      # to unzip the dataset\n",
        "import json                         # to read structured raw text\n",
        "import pandas as pd                 # for dataset management\n",
        "from sys import getsizeof           # to check size of a variable (use %whos to see loist of all variables)\n",
        "import gc                           # garbage collector for memory management\n",
        "import pickle                       # used to store pickled versions of the datasets\n",
        "from tqdm import tqdm               # to show progress of operations\n",
        "import glob                         # to use regular expressions & paths\n",
        "import shutil                       # to easily copy files\n",
        "from urllib.parse import urlparse   # to retrieve specified fields from url string\n",
        "import ast                          # to convert the string representations to dictionaries\n",
        "import requests                     # to contact wikidata server\n",
        "import csv                          # to build kg for addressa news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xqxhn0rdOimE",
        "outputId": "41089671-2e34-4f45-8acf-9bd3215b91a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: protobuf>=4.22.3 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.23.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.30.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qc_ooS6JtTu"
      },
      "source": [
        "## 2. Environment setup\n",
        "- Clone the GitHub repository of the project to get the code\n",
        "- Position the working directory in the appropriate folder\n",
        "- Mount the Google Drive in order to access the dataset (contact the team to have access to it)\n",
        "- Move the dataset in the appropriate folder to be accessed by the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBfi3KcS3dF_",
        "outputId": "766a9257-61e0-4fd4-9dc6-26737ac9b4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KRED-Reccomendation-System'...\n",
            "remote: Enumerating objects: 369, done.\u001b[K\n",
            "remote: Counting objects: 100% (269/269), done.\u001b[K\n",
            "remote: Compressing objects: 100% (184/184), done.\u001b[K\n",
            "remote: Total 369 (delta 149), reused 176 (delta 80), pack-reused 100\u001b[K\n",
            "Receiving objects: 100% (369/369), 7.42 MiB | 9.40 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "/content/KRED-Reccomendation-System\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot open '/content/drive/MyDrive/AdressaProcessed/behavior-day1.gsheet' for reading: Operation not supported\n",
            "\n",
            "\n",
            "Stage completed in 6.24 minutes\n"
          ]
        }
      ],
      "source": [
        "t1 = time()\n",
        "\n",
        "# Clone the GitHub repository\n",
        "repo = \"KRED-Reccomendation-System\"\n",
        "if os.path.isdir(repo):\n",
        "  !rm -rf {repo}\n",
        "\n",
        "!git clone https://github.com/micolrosini/KRED-Reccomendation-System.git\n",
        "\n",
        "# Enter the correct directory (with main.py)\n",
        "%cd 'KRED-Reccomendation-System'\n",
        "\n",
        "# Mount Drive with Dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Move the dataset in the correct folder for the code to run\n",
        "%cp -R /content/drive/MyDrive/KRED/KRED/data /content/KRED-Reccomendation-System\n",
        "%cp -R /content/drive/MyDrive/AdressaProcessed /content/KRED-Reccomendation-System/data\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\n\\nStage completed in {str(round((t2-t1)/60, 2))} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pME33dM2KFhn"
      },
      "source": [
        "# **Extension 1: Adressa dataset - Norwegian news dataset - Loading and preprocessing**\n",
        "\n",
        "This experiment uses the Adressa (small: 1 week) dataset which is a Norwegian news dataset including also a field representing the time spent by a useron an article. This information can be exploited to better infer the actual interest the user had towards each article and therefore enrich the model.\n",
        "\n",
        "**Requirements**:\n",
        "In order to run this section it's necessary to execute before sections *1. Import libraries & install the needed packages* and *2. Environment setup* which imports the dataset from Google Drive (section *3. Execution* shoul be skipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHA6y5rLNtr"
      },
      "source": [
        "## 1.1 Unzip dataset\n",
        "\n",
        "*Remark: This section and section 1.2 (together with its sub-sections) can be skipped since the dataset has already been generated and store on a Google Drive clound. It is possible to run directly 1.3*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY6qf8fBKF3x",
        "outputId": "66f5ed14-a72c-4080-948a-cdd651f06473"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Stage completed in 89.72 seconds\n"
          ]
        }
      ],
      "source": [
        "t1 = time()\n",
        "\n",
        "# UNZIP THE DATASET\n",
        "\n",
        "# File structured as:\n",
        "#   AdressaSMALL.tar.gz --> one_week.tar --> one_week (folder) --> 7 files, one for each day of the week\n",
        "\n",
        "fname = \"AdressaSMALL.tar.gz\"               # dataset compressed file name\n",
        "datapath = \"./data/\"                        # dataset location\n",
        "tar = tarfile.open(datapath+fname, \"r:gz\")  # open the .tar.gz file\n",
        "tar.extractall(path=datapath)               # extract one_week folder and place it in ./data\n",
        "tar.close()                                 # close the file handle\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\n\\nStage completed in {str(round(t2-t1, 2))} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7sNr2Z8Sp8z"
      },
      "source": [
        "## Insights about the dataset attributes:\n",
        "\n",
        "The dataset is mostly composed of 2 types of rows (namely \"Long row\" and \"Short row\") structured as json formatting (read in memory as dictionaries).\n",
        "\n",
        "The documentation of the dataset offers a description of most of the present fields.\n",
        "\n",
        "The Short Row is (almost always) a subset of Long Row.\n",
        "\n",
        "\n",
        "\n",
        "| Attribute | Type | Description | Long row | Short row |\n",
        "|:--------------|:-----------|:------------|:------------|:------------|\n",
        "| eventId | int | The identifier used to differentiate distinct events from the same user | True | True |\n",
        "| activeTime | int | The active time on a page in seconds, if known | True | True |\n",
        "| os | string | Operating system that the user used when log in. | True | True\n",
        "| referrerUrl | sting | The URL of the referrer page. | True | False\n",
        "| deviceType | string | The type of the device. | True | True\n",
        "| sessionStart | boolean | Indicates whether the event is considered as the first event in session | True | True |\n",
        "| sessionStop | boolean | Indicates whether the event is considered as the last event in session | True | True |\n",
        "| userId | string | The cross-site user identifier which can be used to differentiate devices/browsers, or identify different subscription users by the user id | True | True |\n",
        "| category | sting | The category of the news article. | False | False |\n",
        "| city | string | The city name inferred from the IP address. | True | True |\n",
        "| country | string | The country code inferred from the userâ€™s IP address. | True | True |\n",
        "| region | string | The region code inferred from the IP address. | True |  True |\n",
        "| time | int | The time of event, measured in Unix time. | True | True |\n",
        "| canonicalUrl | sting | Canonical URL as calculated based on incoming events and the fetched page content. | True | False |\n",
        "| documentId (present as \"id\") | string | The document id. This will be the same for different URLs | True | False |\n",
        "| title | string | The title of the article. | True | False |\n",
        "| keywords | list | The keywords of the article. | True | False |\n",
        "| namedEntities | list | The named entities of the article, including their types, counts and weights. | False | False |\n",
        "| author | string | The author of the article. | True | False |\n",
        "| publishTime | string | The publish time of the article. | True | False |\n",
        "| profile | Array of object | A set of items which are extracted or generated from the page content. Usually a string or keywords or Named Entities from the page | True | False |\n",
        "| item\\* | sting | Item extracted or generated from the page content. Usually a string or keyword extracted from the page. | True* | False |\n",
        "| referrerHostClass | string | NOT PRESENT IN THE DOCUMENTATION | True | True |\n",
        "| url | string | NOT PRESENT IN THE DOCUMENTATION | True | True |\n",
        "| referrerSearchEngine | string | NOT PRESENT IN THE DOCUMENTATION | True | False |\n",
        "| query | string | NOT PRESENT IN THE DOCUMENTATION | ? | ? |\n",
        "| referrerSocialNetwork | ? | NOT PRESENT IN THE DOCUMENTATION | ? | ? |\n",
        "| category1 | ? | NOT PRESENT IN THE DOCUMENTATION | ? | ? |\n",
        "| referrerQuery | ? | NOT PRESENT IN THE DOCUMENTATION | ? | ? |\n",
        "\n",
        "(*) Item is not a field of the rows, but is a subfield of profile in the long rows\n",
        "\n",
        "### Conclusions\n",
        "Attributes to keep:\n",
        "- eventId --> primary key of the table\n",
        "- time --> to get a ordering & needed by behaviours.tsv\n",
        "- userId --> needed by behaviours.tsv\n",
        "- url --> needed by both news.ts used as News ID\n",
        "- profile --> needed by news.tsv to extract entities\n",
        "- category1 --> needed by news.tsv to extract entities\n",
        "- keywords --> needed by news.tsv to extract entities\n",
        "- title --> needed by news.tsv to extract entities\n",
        "- activeTime --> needed to recreate user behavior.tsv\n",
        "- sessionStart --> needed to recreate user history (possibly removable)\n",
        "- sessionStop --> needed to recreate user history (possibly removable)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gPq32iAzmHf"
      },
      "source": [
        "## 1.2 Read dataset in a proper structure\n",
        "\n",
        "The dataset is composed of 7 files, one for each day of a week. Files are in .txt extension in which rows are strings with json formatting.\n",
        "\n",
        "The 7 daily files are too big (w.r.t. Colab 12GB of RAM) to be directly loaded and processed in RAM within a single operation.\n",
        "\n",
        "The procedure is the following:\n",
        "Block 1.2.1\n",
        "1. Read one daily file at the time\n",
        "2. For each daily file, process it in batches of X rows at the time\n",
        "3. The file is initially read as a list of strings (rows), then exploit JSON formatting to transition to a list of dictionaries and eventually obtain a pandas dataframe\n",
        "4. In the process at point 3. many other transformations are applied in order to properly clean the data and reorganize the dataframe schema\n",
        "5. Each dataframe obtained at point 4. (of the given batch size X) is saved as a segment of the given day\n",
        "6. When all the segments of a day have been processed they are again loaded in memory and reassembled as a single day to be finally stored as a single .csv file\n",
        "\n",
        "Block 1.2.2\n",
        "7. All the 7 full daily .csv files are read and merged into a single weekly file\n",
        "8. The weekly file is saved as a .csv file\n",
        "9. All the daily and weekly files are also copied to the mounted Google Drive for future use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnSTTgsMfJSe",
        "outputId": "089d15c3-e3c1-4ab8-da54-d55d3d434be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing file 1 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 11.45 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1513739)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 138.78 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 14.2 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1513739)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 139.51 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 12.67 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1513739)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 146.41 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 15.45 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1400000] (total rows: 1513739)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 158.01 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 21.09 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 5 with indices [1400000,1513739] (total rows: 1513739)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 68.29 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 7.47 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 1:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day1.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 43.73 seconds\n",
            "\n",
            "Preprocessing file 2 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 13.36 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1613128)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 127.76 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 13.99 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1613128)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 135.89 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 9.12 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1613128)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 147.81 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 18.98 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1400000] (total rows: 1613128)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 162.7 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 22.78 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 5 with indices [1400000,1613128] (total rows: 1613128)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 113.35 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 15.95 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 2:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day2.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 49.6 seconds\n",
            "\n",
            "Preprocessing file 3 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 9.47 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1087223)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 128.4 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 15.43 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1087223)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 156.99 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.95 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1087223)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 158.43 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.4 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1087223] (total rows: 1087223)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 34.77 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 1.94 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 3:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day3.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 36.21 seconds\n",
            "\n",
            "Preprocessing file 4 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 15.31 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1496417)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 143.34 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 20.01 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1496417)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 163.15 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.54 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1496417)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 174.69 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 28.18 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1400000] (total rows: 1496417)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 179.13 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 26.4 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 5 with indices [1400000,1496417] (total rows: 1496417)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 68.93 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 6.62 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 4:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day4.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 61.1 seconds\n",
            "\n",
            "Preprocessing file 5 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 11.48 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1327429)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 138.54 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.27 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1327429)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 152.61 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.98 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1327429)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 159.76 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.85 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1327429] (total rows: 1327429)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 135.48 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 17.51 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 5:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day5.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 47.4 seconds\n",
            "\n",
            "Preprocessing file 6 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 12.87 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1648346)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 133.52 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 17.25 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1648346)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 148.59 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 15.97 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1648346)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 150.3 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 18.82 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1400000] (total rows: 1648346)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 155.58 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 19.29 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 5 with indices [1400000,1648346] (total rows: 1648346)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 121.39 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 15.01 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 6:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day6.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 52.87 seconds\n",
            "\n",
            "Preprocessing file 7 of 7\n",
            "\t- Reading file from disk\n",
            "\t- Transforming file into a list of rows\n",
            "\t  Stage completed in 8.43 seconds\n",
            "\n",
            "- Processing batch 1 with indices [0,350000] (total rows: 1356987)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 135.94 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 18.57 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 2 with indices [350000,700000] (total rows: 1356987)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 146.16 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 11.76 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 3 with indices [700000,1050000] (total rows: 1356987)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 131.94 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 11.3 seconds\n",
            "\n",
            "\n",
            "\n",
            "- Processing batch 4 with indices [1050000,1356987] (total rows: 1356987)\n",
            "\t- Transforming the json-formatted list of rows into a list of dictionaries\n",
            "\t- Casting JSON strings to dictionaries progress: 100.0%\n",
            "\t- Transforming the list of dictionaries into a pandas dataframe\n",
            "\t- Projecting on the needed dataframe attributes\n",
            "\t  Stage completed in 118.75 seconds\n",
            "\n",
            "\t- Saving the segment on disk\n",
            "\t  Stage completed in 7.34 seconds\n",
            "\n",
            "\n",
            "- All batches have been processed and saved\n",
            "\n",
            "Reassembling segments of day 7:\n",
            "- Reading all the .CSV segments to build final dataset\n",
            "\t- Unpickling segments: 100.0%\n",
            "\t- Concatenating segments into a single dataframe\n",
            "\t- Freeing memory\n",
            "- Saving final dataset as: ./data/pickled_dataset/day7.csv\n",
            "- Saved successfully\n",
            "- Removing segments\n",
            "  Stage completed in 31.75 seconds\n",
            "\n",
            "\n",
            "\n",
            "Stage completed in 4853.15 seconds\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "NEW VERSION: Loops on the 7 days authomatically\n",
        "\n",
        "Remark: pickle has been abandoned due to lack of RAM, CSV format is in use, but\n",
        "        the old variable and path names including \"pickle\" remain for sake of simplicity\n",
        "        (could be refactored)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "t_start = time()\n",
        "\n",
        "days = [1,2,3,4,5,6,7]\n",
        "\n",
        "for d in days:\n",
        "  SELECTED_DAY = d                            # loop on all the days\n",
        "\n",
        "  BATCH_SIZE = 350000                         # how many lines to process at the time\n",
        "\n",
        "  if SELECTED_DAY not in range(1,8):          # check selection is in [1,2,3,4,5,6,7]\n",
        "    exit(-1)\n",
        "\n",
        "  VERBOSE = False                              # print some additional info\n",
        "\n",
        "  SAVE_MODE = \"csv\"                           # use pickle only with more than 12 GB of ram\n",
        "  if SAVE_MODE not in ['csv', 'pkl']:         # check selection is valid\n",
        "    exit(-1)\n",
        "\n",
        "  DELETE_FINAL_DATAFRAME = True               # clear final df (whole day)\n",
        "\n",
        "  datapath = \"./data/\"                                        # just a reminder\n",
        "  adressa_datapath = datapath + \"one_week/\"                   # build the path of the unzipped Adressa dataset\n",
        "  pickled_dataset_path = datapath + \"pickled_dataset/\"        # build the path where the csv datasets will be stored\n",
        "  segments_dataset_path = pickled_dataset_path + \"segments/\"  # build the path where the daily csv segments will be stored\n",
        "\n",
        "  filenames = os.listdir(adressa_datapath)    # get the list of the 7 daily filenames\n",
        "  inputFileName = filenames[SELECTED_DAY-1]\n",
        "\n",
        "  # Complete list of dataset attributes:\n",
        "  \"\"\"\n",
        "  | eventId | city | activeTime | url | referrerHostClass | region | time\n",
        "  | userId | sessionStart | deviceType | sessionStop | country | os | referrerUrl\n",
        "  | profile | category1 | canonicalUrl | publishtime | keywords | id | title\n",
        "  | author | referrerSearchEngine | referrerSocialNetwork | referrerQuery | query |\n",
        "  \"\"\"\n",
        "  # Keep a subset of attributes\n",
        "  attributes = [\n",
        "                'eventId',        # primary key of the table\n",
        "                'time',           # to get a ordering & needed by behaviours.tsv\n",
        "                'userId',         # needed by behaviours.tsv\n",
        "\n",
        "                #'id',            # newsId, needed by both news.tsv & behaviours.tsv BUT DROPPED since it not complete (url will work also as newsId, i.e. primary key for news)\n",
        "                'url',            # needed by both news.ts (AND read line above)\n",
        "                'profile',        # needed by both news.ts\n",
        "                'category1',      # needed by both news.ts\n",
        "                'keywords',       # needed by both news.ts\n",
        "                'title',          # needed by both news.ts\n",
        "\n",
        "                'activeTime',     # needed to recreate user history --> compute median reading time --> compute appreciation index\n",
        "                'sessionStart',   # needed to recreate user history --> compute median reading time --> compute appreciation index (possibly removable, but it's just a Boolean, doesn't take much space)\n",
        "                'sessionStop',    # needed to recreate user history --> compute median reading time --> compute appreciation index (possibly removable, but it's just a Boolean, doesn't take much space)\n",
        "                ]\n",
        "\n",
        "\n",
        "  # =================================== START ===================================\n",
        "\n",
        "  print(f\"Preprocessing file {SELECTED_DAY} of {len(filenames)}\")\n",
        "\n",
        "\n",
        "  # 1. Read single file from disk\n",
        "  # 2. split in rows\n",
        "  # 3. free memory\n",
        "  # 4. remove empty rows\n",
        "\n",
        "  t1 = time()\n",
        "\n",
        "  dataset = list()\n",
        "\n",
        "  with open(adressa_datapath + inputFileName) as daily_file:  # open the selected file\n",
        "\n",
        "    print('\\t- Reading file from disk')\n",
        "    fileContents = daily_file.read()                          # load the file into a single big string\n",
        "\n",
        "    print('\\t- Transforming file into a list of rows')\n",
        "    dataset = dataset + fileContents.split(\"\\n\")              # split the whole file into lines & save the obtained rows into a list\n",
        "\n",
        "    del fileContents                                          # free the memory\n",
        "    gc.collect()                                              # call the garbage collector to force variables deletion\n",
        "\n",
        "    dataset = list(filter(lambda a: a != '', dataset))        # lambda expression to keep only non-empty rows\n",
        "\n",
        "  t2 = time()\n",
        "  print(f\"\\t  Stage completed in {str(round(t2-t1, 2))} seconds\")\n",
        "\n",
        "  # 5. Loop on batches\n",
        "  # 6. Restrict dataset size\n",
        "  # 7. Cast to list of dictionaries\n",
        "  # 8. Cast to datadrame\n",
        "  # 9. Project on attributes\n",
        "  # 10. Reorder & rename columns\n",
        "  # 11. Free memory\n",
        "\n",
        "  t1 = time()\n",
        "\n",
        "  rowsOfTheDay = len(dataset)                             # total rows of selected day\n",
        "  segmentsProcessed = 0                                   # how many batches have already been processed so far\n",
        "\n",
        "  while segmentsProcessed*BATCH_SIZE < rowsOfTheDay:      # as long as there are still rows to process\n",
        "    startIndex = segmentsProcessed*BATCH_SIZE             # reposition start index\n",
        "    stopIndex = startIndex + BATCH_SIZE                   # reposition end index\n",
        "    if stopIndex > rowsOfTheDay:                          # fix end index out of boundaries of dataframe\n",
        "      stopIndex = rowsOfTheDay\n",
        "\n",
        "    print(f'\\n- Processing batch {segmentsProcessed+1} with indices [{startIndex},{stopIndex}] (total rows: {rowsOfTheDay})')\n",
        "\n",
        "    datasetSmall = dataset[startIndex:stopIndex]          # restrict the dataset size to a batch\n",
        "    gc.collect()                                          # free memory (nothing to free at first iteration)\n",
        "\n",
        "    # Exploit JSON formatting of rows to get a list of dictionaries\n",
        "    print('\\t- Transforming the json-formatted list of rows into a list of dictionaries')\n",
        "    c = 0                                                 # counter to track progression\n",
        "    JSONDataset = list()                                  # new dataset handle\n",
        "    for r in datasetSmall:                                # loop over each single row (string with json formatting)\n",
        "      JSONDataset.append(json.loads(r))                   # each json row is transformed to a dictionary\n",
        "      c += 1                                              # count the row as read\n",
        "      print(f\"\\r\\t- Casting JSON strings to dictionaries progress: {str(round(c/(stopIndex-startIndex)*100, 2))}%\",end='')\n",
        "    print(f\"\\r\\t- Casting JSON strings to dictionaries progress: {str(round(c/(stopIndex-startIndex)*100, 2))}%\")\n",
        "    del datasetSmall                                      # free memory asap\n",
        "    gc.collect()                                          # call the garbage collector to force variables deletion\n",
        "\n",
        "    print('\\t- Transforming the list of dictionaries into a pandas dataframe')\n",
        "    df = pd.DataFrame(JSONDataset)                        # cast list of dictionaries to Pandas DataFrame\n",
        "    del JSONDataset                                       # free memory asap\n",
        "    gc.collect()                                          # call the garbage collector to force variables deletion\n",
        "\n",
        "    print('\\t- Projecting on the needed dataframe attributes')\n",
        "    dfSmall = df[attributes]                              # project onto the needed attributes only\n",
        "    dfSmall = dfSmall.reindex(columns=attributes)         # rearrange columns order\n",
        "\n",
        "    if \"category1\" in dfSmall.columns:                    # rename column \"category\"\n",
        "      dfSmall.rename(columns = {'category1':'category'}, inplace = True)\n",
        "\n",
        "    del df                                                # free memory asap\n",
        "    gc.collect()                                          # call the garbage collector to force variables deletion\n",
        "\n",
        "    t2 = time()\n",
        "    print(f\"\\t  Stage completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    # 12. Save csv version of the segment\n",
        "    # 13. Go back to 5. until all file has been processed\n",
        "\n",
        "    t1 = time()\n",
        "    outputSegmentName = f\"day{SELECTED_DAY}segment{segmentsProcessed+1}.csv\" # e.g. day1segment1.csv\n",
        "    outputPath = segments_dataset_path + outputSegmentName                   # e.g. ./data/pickled_dataset/segments/day1segment1.csv\n",
        "\n",
        "    if not os.path.exists(segments_dataset_path):          # create ./data/pickled_dataset/segments/ if not exists\n",
        "      os.makedirs(segments_dataset_path)\n",
        "\n",
        "    print('\\t- Saving the segment on disk')\n",
        "    dfSmall.to_csv(outputPath, index = False)              # ./data/pickled_dataset/segments/day1segment1.csv\n",
        "    del dfSmall                                            # free the memory asap\n",
        "    gc.collect()                                           # call the garbage collector to force variables deletion\n",
        "\n",
        "    t2 = time()\n",
        "    print(f\"\\t  Stage completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "\n",
        "\n",
        "    segmentsProcessed += 1\n",
        "    print()\n",
        "\n",
        "  print(f\"- All batches have been processed and saved\\n\")\n",
        "\n",
        "  # 14. re-load all .csv segments\n",
        "  # 15. concatenate dataframe segments into a single dataframe\n",
        "  # 16. save final pickled versionn of the day\n",
        "\n",
        "  t1 = time()\n",
        "\n",
        "  print(f\"Reassembling segments of day {SELECTED_DAY}:\")\n",
        "\n",
        "  print('- Reading all the .CSV segments to build final dataset')\n",
        "\n",
        "  segmentNames = glob.glob(segments_dataset_path + '*.csv')         # read all segments full paths\n",
        "\n",
        "  c = 0                                                             # track the progression\n",
        "  segments = list()                                                 # list of segments df that are going to be read\n",
        "  for segmentName in segmentNames:                                  # for each csv segment\n",
        "    print(f\"\\r\\t- Unpickling segments: {str(round(c/len(segmentNames)*100, 2))}%\",end='')\n",
        "    segments.append(pd.read_csv(segmentName, index_col=False))      # read the segment\n",
        "    c += 1\n",
        "  print(f\"\\r\\t- Unpickling segments: {str(round(c/len(segmentNames)*100, 2))}%\")\n",
        "  gc.collect()                                                      # free some memory in case of memory leaks\n",
        "\n",
        "  print(f\"\\t- Concatenating segments into a single dataframe\")\n",
        "  finalDf = pd.concat(segments, ignore_index=True)                  # build final dataset concatenating segments (whole day)\n",
        "  print(f\"\\t- Freeing memory\")\n",
        "  segments.clear()                                                  # free memory asap\n",
        "  del segments                                                      # free memory asap\n",
        "  gc.collect()                                                      # free memory asap\n",
        "\n",
        "  if VERBOSE:                                                       # additional info on screen\n",
        "    print(f\"Dataframe length: {len(finalDf)} rows\")\n",
        "    print(f\"Dataframe size: {str(round(getsizeof(finalDf) /1024 /1024,2))} MB\")\n",
        "    print(finalDf.info())\n",
        "\n",
        "  if SAVE_MODE == 'pkl':                                            # DEPRECATED\n",
        "    outputFileName = f\"day{SELECTED_DAY}.pkl\"\n",
        "    outputPath = pickled_dataset_path + outputFileName\n",
        "    print(f'- Saving final dataset as: {outputPath}')\n",
        "    finalDf.to_pickle(outputPath)\n",
        "\n",
        "  elif SAVE_MODE == 'csv':                                          # TO BE USED\n",
        "    outputFileName = f\"day{SELECTED_DAY}.csv\"\n",
        "    outputPath = pickled_dataset_path + outputFileName               # e.g. /content/KRED-Reccomendation-System/data/pickled_dataset/day1.csv\n",
        "    print(f'- Saving final dataset as: {outputPath}')\n",
        "    finalDf.to_csv(outputPath, index = False)\n",
        "\n",
        "  else:\n",
        "    # Should not happen\n",
        "    print(f'- Failed to save final dataframe')\n",
        "\n",
        "  print(f'- Saved successfully')\n",
        "\n",
        "  if DELETE_FINAL_DATAFRAME:                                        # delete whole day df if not needed immediately\n",
        "    del finalDf                                                     # free memory asap\n",
        "\n",
        "  gc.collect()                                                      # free memory asap\n",
        "\n",
        "  print(f'- Removing segments')\n",
        "  if os.path.isdir(segments_dataset_path):                          # segments folder must be removed\n",
        "    !rm -rf {segments_dataset_path}                                 #  to avoid interactions over different runs\n",
        "\n",
        "  t2 = time()\n",
        "  print(f\"  Stage completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "\n",
        "\n",
        "t_end = time()\n",
        "print(f\"\\n\\nStage completed in {str(round(t_end-t_start, 2))} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs8GUQ0QVKRp"
      },
      "source": [
        "1.2.2 Build final dataset:\n",
        "- load all 7 daily files\n",
        "- build weekly dataset\n",
        "- store weekly dataset in the mounted Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lULCZCe1bVsG",
        "outputId": "a5581d3f-eb70-44b5-b7b1-5c0464ffa3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./data/pickled_dataset/day3.csv\n",
            "Dataframe length: 1087223 rows\n",
            "Dataframe size: 1334.42 MB\n",
            "\n",
            "./data/pickled_dataset/day1.csv\n",
            "Dataframe length: 1513739 rows\n",
            "Dataframe size: 1630.48 MB\n",
            "\n",
            "./data/pickled_dataset/day6.csv\n",
            "Dataframe length: 1648346 rows\n",
            "Dataframe size: 1935.27 MB\n",
            "\n",
            "./data/pickled_dataset/day5.csv\n",
            "Dataframe length: 1327429 rows\n",
            "Dataframe size: 1674.37 MB\n",
            "\n",
            "./data/pickled_dataset/day2.csv\n",
            "Dataframe length: 1613128 rows\n",
            "Dataframe size: 1880.59 MB\n",
            "\n",
            "./data/pickled_dataset/day4.csv\n",
            "Dataframe length: 1496417 rows\n",
            "Dataframe size: 2182.39 MB\n",
            "\n",
            "./data/pickled_dataset/day7.csv\n",
            "Dataframe length: 1356987 rows\n",
            "Dataframe size: 1237.12 MB\n",
            "\n",
            "Dataframe length: 10043269 rows\n",
            "Dataframe size: 11874.65 MB\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10043269 entries, 0 to 10043268\n",
            "Data columns (total 11 columns):\n",
            " #   Column        Dtype  \n",
            "---  ------        -----  \n",
            " 0   eventId       int64  \n",
            " 1   time          int64  \n",
            " 2   userId        object \n",
            " 3   url           object \n",
            " 4   profile       object \n",
            " 5   category      object \n",
            " 6   keywords      object \n",
            " 7   title         object \n",
            " 8   activeTime    float64\n",
            " 9   sessionStart  bool   \n",
            " 10  sessionStop   bool   \n",
            "dtypes: bool(2), float64(1), int64(2), object(6)\n",
            "memory usage: 708.8+ MB\n",
            "None\n",
            "\n",
            "Stage completed in 104.11 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "t1 = time()\n",
        "\n",
        "DELETE = True                                               # cleanup at the end\n",
        "\n",
        "dailyFileNames = glob.glob(pickled_dataset_path + '*.csv')  # get list of daily files paths\n",
        "\n",
        "dayilyDfs = list()                                          # read all daily files into a list\n",
        "progress = 0\n",
        "for name in dailyFileNames:\n",
        "  print(name)\n",
        "  dayilyDfs.append(pd.read_csv(name, index_col=False))\n",
        "  print(f\"Dataframe length: {len(dayilyDfs[progress])} rows\")\n",
        "  print(f\"Dataframe size: {str(round(getsizeof(dayilyDfs[progress]) /1024 /1024,2))} MB\\n\")\n",
        "  progress += 1\n",
        "\n",
        "weeklyDf = pd.concat(dayilyDfs, ignore_index=True)          # concatenate all daily files into a weekly file\n",
        "print(f\"Dataframe length: {len(weeklyDf)} rows\")\n",
        "print(f\"Dataframe size: {str(round(getsizeof(weeklyDf) /1024 /1024,2))} MB\\n\")\n",
        "print(weeklyDf.info())\n",
        "\n",
        "\n",
        "if DELETE:                                                   # cleanup\n",
        "  dayilyDfs.clear()\n",
        "  del(dayilyDfs)\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "outputFileName = \"week.csv\"                         # Save weekly df on disk\n",
        "outputPath = pickled_dataset_path + outputFileName  # e.g. /content/KRED-Reccomendation-System/data/pickled_dataset/week.csv\n",
        "print(f'- Saving final dataset as: {outputPath}')\n",
        "weeklyDf.to_csv(outputPath, index = False)          # WARNING: files is 9+GB --> takes a lot of time\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/week.csv\", \"/content/drive/MyDrive/AdressaProcessed/week.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day1.csv\", \"/content/drive/MyDrive/AdressaProcessed/day1.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day2.csv\", \"/content/drive/MyDrive/AdressaProcessed/day2.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day3.csv\", \"/content/drive/MyDrive/AdressaProcessed/day3.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day4.csv\", \"/content/drive/MyDrive/AdressaProcessed/day4.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day5.csv\", \"/content/drive/MyDrive/AdressaProcessed/day5.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day6.csv\", \"/content/drive/MyDrive/AdressaProcessed/day6.csv\")\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/pickled_dataset/day7.csv\", \"/content/drive/MyDrive/AdressaProcessed/day7.csv\")\n",
        "\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30RHOSN7na6V"
      },
      "source": [
        "##1.3 Read & use the preprocessed Adressa dataset\n",
        "\n",
        "*Remark: This block doesn't necessarily require the execution of blocks 1.1, 1.2 and their sub-blocks since the preprocessed datasets have already been stored on Google Drive*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0-FgY-UsJQZ"
      },
      "source": [
        "1.3.1 Load the desired input\n",
        "\n",
        "Can load one of the seven daily files or the week file (latter to be used for final model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-laijz6ezS1r"
      },
      "outputs": [],
      "source": [
        "t1 = time()\n",
        "\n",
        "SELECTED_INPUT = 'week.csv'         # select the needed input file\n",
        "VERBOSE = True                      # to print additional information\n",
        "\n",
        "if SELECTED_INPUT not in ['week.csv', 'day1.csv', 'day2.csv', 'day3csv', 'day4.csv', 'day5.csv', 'day6.csv', 'day7.csv']:\n",
        "  exit(-1)\n",
        "\n",
        "inputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'  # use this copy in local disk, never modify version in Drive folder\n",
        "inputPath = inputFolder + SELECTED_INPUT                                    # e.g. /content/KRED-Reccomendation-System/data/AdressaProcessed/day1.csv\n",
        "\n",
        "inputDf = pd.read_csv(inputPath, index_col=False)\n",
        "print(f'Loaded file: {SELECTED_INPUT}')\n",
        "if VERBOSE:\n",
        "  print(f\"Dataframe length: {len(inputDf)} rows\")\n",
        "  print(f\"Dataframe size: {str(round(getsizeof(inputDf) /1024 /1024,2))} MB\\n\")\n",
        "  print(inputDf.info())\n",
        "  # print(inputDf[0:10])\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcTxFvnNtWR-"
      },
      "source": [
        "1.3.2 Work towards behaviours.tsv format\n",
        "- compute mean & standard deviation of reading time (for each user)\n",
        "- classify each entry as element to be put into positive or negative list\n",
        "- build dataset structured as behaviours.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6oofZhNtVlK"
      },
      "outputs": [],
      "source": [
        "# Build the columns of mean active time for each user\n",
        "# --> obtain: userDf\n",
        "t1 = time()\n",
        "\n",
        "VERBOSE = True\n",
        "\n",
        "userTimeDf = inputDf.groupby(['userId'])[\"activeTime\"].aggregate(['min', 'max', 'median', 'mean', 'std'])     # for each userId obtain mean & std dev of activeTime (+ other aggregations useful for data exploration)\n",
        "userTimeDf.rename(columns = {'mean':'userMeanActiveTime', 'std':'userStdDevActiveTime'}, inplace = True)      # rename useful columns\n",
        "userTimeDf = userTimeDf[['userMeanActiveTime', 'userStdDevActiveTime']]                                       # select columns for the join (userId isn't a column, but the index of the table)\n",
        "\n",
        "userDf = inputDf.join(userTimeDf, on='userId', validate='m:1')                                                # left-join over userId, validate n:1 multiplicity of key attributes\n",
        "\n",
        "if VERBOSE:\n",
        "  print(userTimeDf.info())\n",
        "  print(userDf.info())\n",
        "\n",
        "del userTimeDf                                                                  # free memory\n",
        "gc.collect()                                                                    # free memory\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEzpXxCKRXMf"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "1. Filter out useless attributes\n",
        "2. Filter out users whose activeTime is always NaN --> not informative (do not allow computation of userMeanActiveTime)\n",
        "3. Set all remaining NaN activeTimes to -1 (this tell us that the user immediately closed the page, in less than a second)\n",
        "4. Classify each row as element for a Positive List or Negative List\n",
        "6. Build positive & negative lists and drop users with empty lists\n",
        "7. Build a new datadrame with same schema of behaviors.tsv and save it\n",
        "\"\"\"\n",
        "\n",
        "t1 = time()\n",
        "\n",
        "VERBOSE = True\n",
        "DOUBLE_BOUNDARY = True\n",
        "\n",
        "# 1. FILTER OUT USELESS ATTRIBUTES\n",
        "#   Select only the columns needed by behaviours.tsv\n",
        "#   ---> obtain: behaviorDf1\n",
        "\n",
        "print(\"\\n1. Projecting on columns subset\")\n",
        "attributes = ['userId', 'time', 'url', 'activeTime', 'userMeanActiveTime', 'userStdDevActiveTime']\n",
        "behaviorDf1 = userDf[attributes]\n",
        "if VERBOSE:\n",
        "  print(\"\\n\\n=============== STEP 1 ===============\\n\")\n",
        "  print(behaviorDf1.info())\n",
        "\n",
        "\n",
        "# 2. FILTER OUT NON-INFORMATIVE USERS\n",
        "#   Keep only a subset of rows where the users have at least one activeTime not NaN (i.e. discard users for which can't compute mean active time)\n",
        "#   ---> obtain: behaviorDf2\n",
        "\n",
        "print(\"\\n2. Removing users without at least 1 activeTime!=NaN\")\n",
        "behaviorDf2 = behaviorDf1[behaviorDf1['userMeanActiveTime'].notna()]   # filter out users whose activeTime = NaN for all rows\n",
        "behaviorDf2 = behaviorDf2[behaviorDf2['userStdDevActiveTime'].notna()]   # filter out users whose activeTime = NaN for all rows\n",
        "if VERBOSE:\n",
        "  print(\"\\n\\n=============== STEP 2 ===============\\n\")\n",
        "  print(f'Dropped {len(behaviorDf1)-len(behaviorDf2)} rows of the initial {len(behaviorDf1)}')\n",
        "  print(behaviorDf2.info())\n",
        "\n",
        "\n",
        "# 3. FILL \"MISSING\" DATA\n",
        "#   Whenever activeTime = NaN it means that activeTime < 1second therefore it can still be treated as a low active time and flagged with a -1 to make it numerical\n",
        "#   ---> obtain: behaviorsDf3\n",
        "#   should this approach have negative impact on the performances they can still be dropped instead (since are flagged with a unique value)\n",
        "print(\"\\n3. Filling remaining NaN activeTime with -1\")\n",
        "behaviorDf3 = behaviorDf2\n",
        "behaviorDf3['activeTime'] = behaviorDf3['activeTime'].fillna(-1)\n",
        "# behaviorDf3 = behaviorDf3[behaviorDf3['activeTime'].notna()]           # use this row instead to drop rows with -1\n",
        "if VERBOSE:\n",
        "  print(\"\\n\\n=============== STEP 3 ===============\\n\")\n",
        "  filled = len(behaviorDf3[behaviorDf3['activeTime']==-1])\n",
        "  print(f'Filled {filled} rows of the {len(behaviorDf3)} total rows')\n",
        "  print(behaviorDf3.info())\n",
        "\n",
        "\n",
        "# 4. CLASSIFY EACH ROW (i.e. each clicked news by each user) AS POSITIVE/NEGATIVE LIST ELEMENT\n",
        "#   If activeTime <  meanUserActiveTime - 1 * userStdDevActiveTime --> Negative List\n",
        "#   If activeTime >= meanUserActiveTime - 1 * userStdDevActiveTime --> Negative List\n",
        "#   alternatively use a range: positive if inside range [mean - stdDev ; mean + stdDev] & negative otherwise\n",
        "#   ---> obtain: behaviorDf4\n",
        "\n",
        "print(\"\\n4. Classifying clicks\")\n",
        "behaviorDf4 = behaviorDf3\n",
        "\n",
        "if DOUBLE_BOUNDARY:\n",
        "  behaviorDf4['condition1'] = (behaviorDf4['activeTime'] >= (behaviorDf4['userMeanActiveTime'] - behaviorDf4['userStdDevActiveTime']))\n",
        "  behaviorDf4['condition2'] = (behaviorDf4['activeTime'] <= (behaviorDf4['userMeanActiveTime'] + behaviorDf4['userStdDevActiveTime']))\n",
        "  behaviorDf4['positiveElement'] = (behaviorDf4['condition1'] == behaviorDf4['condition2'])\n",
        "else:\n",
        "  behaviorDf4['positiveElement'] = (behaviorDf4['activeTime'] >= (behaviorDf4['userMeanActiveTime'] - behaviorDf4['userStdDevActiveTime']))\n",
        "attrib = ['userId', 'time', 'url', 'positiveElement']\n",
        "behaviorDf4 = behaviorDf4[attrib]\n",
        "\n",
        "\n",
        "\n",
        "#   (DEPRECATED approach below)\n",
        "#   Add two columns with satisfaction indices as satisfactionIndexMean   = activeTime / userMeanActiveTime\n",
        "#                                                satisfactionIndexMedian = activeTime / userMedianActiveTime\n",
        "#   Normalize these indices in range [0,1] to make them comparable with each other --> not necessary since it's just used for ordering\n",
        "# behaviorDf4['satisfactionIndexMean']   = userDf['activeTime'] / userDf['userMeanActiveTime']      # compute a statistic based on the mean active time of each user\n",
        "# Indices normalization\n",
        "# minSatisfactionIndexMean = behaviorDf4['satisfactionIndexMean'].min()\n",
        "# maxSatisfactionIndexMean = behaviorDf4['satisfactionIndexMean'].max()\n",
        "# behaviorDf4['satisfactionIndexMeanNormalized'] = (behaviorDf4['satisfactionIndexMean'] - minSatisfactionIndexMean) / (maxSatisfactionIndexMean - minSatisfactionIndexMean)\n",
        "\n",
        "\n",
        "if VERBOSE:\n",
        "  print(\"\\n\\n=============== STEP 4 ===============\\n\")\n",
        "  posCount = len( behaviorDf4[behaviorDf4['positiveElement'] == True] )\n",
        "  negCount = len( behaviorDf4[behaviorDf4['positiveElement'] == False] )\n",
        "  print(f'{posCount} rows classified as positive clicks ({str(round(posCount/len(behaviorDf4)*100, 2))}%)')\n",
        "  print(f'{negCount} rows classified as negative clicks ({str(round(negCount/len(behaviorDf4)*100, 2))}%)')\n",
        "  print(behaviorDf4.info())\n",
        "\n",
        "\n",
        "# (DEPRECATED --> This step is conducted manually)\n",
        "# 5. GENERATE POSITIVE & NEGATIVE LISTS FOR EACH USER\n",
        "#   Select top and bottom N elements accordin to the ranking given by the indices\n",
        "#   (if picking top N they have same score, pick first the ones with the lowest appreciation for all the users (need another table to join)\n",
        "#   , vice versa for bottom N)\n",
        "#   ---> obtain: behaviorsDf5\n",
        "#print(\"\\n5. Generating positive & negative lists\")\n",
        "#behaviorDf5 = behaviorDf4\n",
        "#if VERBOSE:\n",
        "#  print(\"\\n\\n=============== STEP 5 ===============\\n\")\n",
        "#  print(behaviorDf5.info())\n",
        "\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcQwU-YHZJeN"
      },
      "outputs": [],
      "source": [
        "t1 = time()\n",
        "\n",
        "# TODO:\n",
        "#       convert timestamp to datetime (if necessary, i.e. in case the model can't read the current timestamp formatting)\n",
        "#       print with index & without header columns names\n",
        "#       check order of positive / negative lists\n",
        "#       suffix of negative list\n",
        "#       cast url and userId to a code?\n",
        "\n",
        "updateFreq = 500\n",
        "GC_FREQ = 1000\n",
        "VERBOSE = True\n",
        "\n",
        "\n",
        "behaviorDf5 = pd.DataFrame(columns=['userId', 'timestamp', 'positiveList', 'negativeList'])         # final dataframe initialization\n",
        "\n",
        "uniqueUsersDf = behaviorDf4['userId'].unique()                                                      # list of unique userIds\n",
        "amountOfUsers = len(uniqueUsersDf)                                                                  # count of unique userIds\n",
        "totRowsProcessed = 0                             # count how many rows from behaviorDf4 have been processed\n",
        "inputRowsKept = 0                                # count how many rows from behaviorDf4 have been kept and condensed into pos/neg lists\n",
        "totRows = len(behaviorDf4)                       # tot rows to be processed from behaviorDf4\n",
        "progress = 0                                     # track progress of behaviorDf5 creation (row by row) --> i.e. how many users have been processed and kept\n",
        "usersDropped = 0                                 # count how many users dropped due to empty lists\n",
        "\n",
        "\n",
        "print(f'(printing updates every: {updateFreq} users - tot users: {amountOfUsers})\\n')\n",
        "\n",
        "for usr in uniqueUsersDf:                                                                           # for each user\n",
        "  if (progress+usersDropped) % updateFreq == 0:                                                     # print every X users processed (to not impact on performances)\n",
        "    print(f\"\\rProcessing: {str(round((progress+usersDropped)/amountOfUsers*100, 2))}% - Elapsed time: {str(round(time()-t1, 2))}s\",end='')\n",
        "  userClicksDf = behaviorDf4[behaviorDf4['userId'] == usr]                                          # get the user's clicks\n",
        "  tstamp = (userClicksDf.sort_values(by=['time'], ascending=True).head(1)).iloc[0]['time']          # get the earlier timestamp of that user's clicks\n",
        "\n",
        "  positiveClicks = (userClicksDf[userClicksDf['positiveElement'] == True])['url'].unique().tolist() # get positive list\n",
        "  negativeClicks = (userClicksDf[userClicksDf['positiveElement'] == False])['url'].unique().tolist()# get negative list\n",
        "  commonElements = set(positiveClicks).intersection(negativeClicks)                                 # check if the lists have elements in common\n",
        "  if len(commonElements) > 0:                                                                       # if they have, remove them from the longer list\n",
        "    for elem in commonElements:\n",
        "      if len(negativeClicks) >= len(positiveClicks):\n",
        "        negativeClicks.remove(elem)\n",
        "      else:\n",
        "        positiveClicks.remove(elem)\n",
        "  commonElements = set(positiveClicks).intersection(negativeClicks)                                 # double check if some common elements still exist (redundandt)\n",
        "  if len(commonElements) > 0:\n",
        "    print(f\"\\nWARNING: overlapping positive & negative lists at row {progress}\")\n",
        "    print(commonElements)\n",
        "\n",
        "  totRowsProcessed += len(userClicksDf)                                                             # keep track of how many rows have been read from input\n",
        "\n",
        "  if len(positiveClicks) == 0 or len(negativeClicks) == 0:                                          # if at least a list is empty --> discard user\n",
        "    usersDropped += 1\n",
        "  else:                                                                                             # else add new row and increase counter progress\n",
        "    behaviorDf5.loc[progress] = [usr, tstamp, positiveClicks, negativeClicks]\n",
        "    inputRowsKept += len(positiveClicks)                                                            # keep count of how many clicks have been kept\n",
        "    inputRowsKept += len(negativeClicks)\n",
        "    progress += 1\n",
        "\n",
        "  del positiveClicks                                                                                # delete all temporary dfs\n",
        "  del negativeClicks\n",
        "  del userClicksDf\n",
        "  del commonElements\n",
        "  if (progress+usersDropped) % GC_FREQ == 0:\n",
        "    gc.collect()                                                                                    # free memory every X users\n",
        "\n",
        "print(f\"\\rProcessing: {str(round((progress+usersDropped)/amountOfUsers*100, 2))}%\")\n",
        "\n",
        "print(f'Tot input rows processed: {totRowsProcessed} wrt to total input rows: {len(behaviorDf4)}')\n",
        "print(f'Dropped input rows: {totRowsProcessed-inputRowsKept} (due to empty lists)')\n",
        "print(f'Users dropped: {usersDropped} wrt to processed users: {usersDropped+progress}(due to empty lists)')\n",
        "\n",
        "\n",
        "\n",
        "# SAVE ON DISK & ON MOUNTED DRIVE:\n",
        "\n",
        "DELETE = False                                                # cleanup dataframe at the end?\n",
        "\n",
        "outputFileName = SELECTED_INPUT                               # usually \"day1.csv\" or \"week.csv\"\n",
        "outputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'\n",
        "outputPath = outputFolder + 'behavior-' + outputFileName      # '/content/KRED-Reccomendation-System/data/AdressaProcessed/behavior-day1.csv'\n",
        "print(f'\\nSaving final dataset as: {outputPath}')\n",
        "behaviorDf5.to_csv(outputPath, index = True)\n",
        "\n",
        "if DELETE:                                                    # cleanup\n",
        "  print('\\n Deleting final dataframe')\n",
        "  behaviorDf5.clear()\n",
        "  del(behaviorDf5)\n",
        "  gc.collect()\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\"+'behavior-' + outputFileName\n",
        "print(f'Copying final dataset into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)\n",
        "\n",
        "if VERBOSE:\n",
        "  print(behaviorDf5)\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIesnGceSWYw"
      },
      "source": [
        "1.3.2.1  Build final weekly dataset *behavior-week.tsv*\n",
        "\n",
        "- Read the 7 daily behavior files: behavior-day1.csv, behavior-day2.csv, ...\n",
        "- Reshape the columns properly (there was an error in the formatting of history and positive/negative lists)\n",
        "- Concatenate the 7 files into one single file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhNm22vWRseO",
        "outputId": "e08b7d96-ab50-428e-8940-fadb58755d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded file: behavior-day1.csv\n",
            "Loaded file: behavior-day2.csv\n",
            "Loaded file: behavior-day3.csv\n",
            "Loaded file: behavior-day4.csv\n",
            "Loaded file: behavior-day5.csv\n",
            "Loaded file: behavior-day6.csv\n",
            "Loaded file: behavior-day7.csv\n",
            "The 7 input dataframes have been loaded correctly\n",
            "\n",
            "Stage completed in 9.06 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# LOAD THE DAILY FULLY PROCESSED DATAFRAMES\n",
        "t1 = time()\n",
        "\n",
        "SELECTED_INPUTS = ['behavior-day1.csv', 'behavior-day2.csv', 'behavior-day3.csv', 'behavior-day4.csv', 'behavior-day5.csv', 'behavior-day6.csv', 'behavior-day7.csv']\n",
        "VERBOSE = False                                                                 # to print additional information\n",
        "\n",
        "inputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'      # source folder\n",
        "\n",
        "dfsToReshape = list()                                                           # list where input DFs will be stored\n",
        "counter = 0\n",
        "for i in SELECTED_INPUTS:\n",
        " inputPath = inputFolder + i                                                    # e.g. /content/KRED-Reccomendation-System/data/AdressaProcessed/behavior-day1.csv\n",
        " dfsToReshape.append(pd.read_csv(inputPath, index_col=False))                   # read all daily behaviors and add them to the list\n",
        " print(f'Loaded file: {SELECTED_INPUTS[counter]}')\n",
        " if VERBOSE:\n",
        "    print(f\"Dataframe length: {len(dfsToReshape[counter])} rows\")\n",
        "    print(f\"Dataframe size: {str(round(getsizeof(dfsToReshape[counter]) /1024 /1024,2))} MB\\n\")\n",
        "    print(dfsToReshape[counter].info())\n",
        " counter += 1\n",
        "\n",
        "print(f\"The {len(dfsToReshape)} input dataframes have been loaded correctly\")\n",
        "if VERBOSE:\n",
        " print(dfsToReshape)\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32XAJZnoVwXH",
        "outputId": "4345b0a7-6e91-4b94-e0f0-2a2d4b0693cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Completed processing of day 1 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 101726 entries, 0 to 101725\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   userId                    101726 non-null  object\n",
            " 1   timestamp                 101726 non-null  int64 \n",
            " 2   userHistory               101726 non-null  object\n",
            " 3   positiveAndNegativeLists  101726 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.1+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 2 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 111067 entries, 0 to 111066\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   userId                    111067 non-null  object\n",
            " 1   timestamp                 111067 non-null  int64 \n",
            " 2   userHistory               111067 non-null  object\n",
            " 3   positiveAndNegativeLists  111067 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.4+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 3 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 76951 entries, 0 to 76950\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   userId                    76951 non-null  object\n",
            " 1   timestamp                 76951 non-null  int64 \n",
            " 2   userHistory               76951 non-null  object\n",
            " 3   positiveAndNegativeLists  76951 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.3+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 4 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 105377 entries, 0 to 105376\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   userId                    105377 non-null  object\n",
            " 1   timestamp                 105377 non-null  int64 \n",
            " 2   userHistory               105377 non-null  object\n",
            " 3   positiveAndNegativeLists  105377 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.2+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 5 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 96323 entries, 0 to 96322\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   userId                    96323 non-null  object\n",
            " 1   timestamp                 96323 non-null  int64 \n",
            " 2   userHistory               96323 non-null  object\n",
            " 3   positiveAndNegativeLists  96323 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.9+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 6 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 114010 entries, 0 to 114009\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   userId                    114010 non-null  object\n",
            " 1   timestamp                 114010 non-null  int64 \n",
            " 2   userHistory               114010 non-null  object\n",
            " 3   positiveAndNegativeLists  114010 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.5+ MB\n",
            "None\n",
            "\n",
            "\n",
            "Completed processing of day 7 behaviors\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 98531 entries, 0 to 98530\n",
            "Data columns (total 4 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   userId                    98531 non-null  object\n",
            " 1   timestamp                 98531 non-null  int64 \n",
            " 2   userHistory               98531 non-null  object\n",
            " 3   positiveAndNegativeLists  98531 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 3.0+ MB\n",
            "None\n",
            "\n",
            "Stage completed in 77.8 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# RESHAPE THE COLUMNS PROPERLY\n",
        "t1 = time()\n",
        "\n",
        "reshapedDfs = list()                                                            # list to store the final version of the daily behavior dataframes\n",
        "counter = 0\n",
        "\n",
        "for df in dfsToReshape:                                                         # for each daily behavior DF\n",
        "\n",
        "  newDf = df.copy(deep=True)                                                    # make a true copy, not just a link\n",
        "  newDf['newPositiveList'] = ''                                                 # add two new columns\n",
        "  newDf['newPosNegList'] = ''\n",
        "  for index, row in newDf.iterrows():                                                                                           # process it row by row\n",
        "    newPosListCol = row['positiveList'].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\")                     # create a userHistory column from the previous positiveList (it's an approximation)\n",
        "    oldPosList = row['positiveList'].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"-1\").replace(\",\", \"\").replace(\" \", \"-1 \")  # append a -1 to all the elements of the positive list (as if they were indeed clicked in KRED)\n",
        "    oldNegList = row['negativeList'].replace(\"'\", \"\").replace(\"[\", \"\").replace(\"]\", \"-0\").replace(\",\", \"\").replace(\" \", \"-0 \")  # append a -0 to all the elements of the negative list (as if they were NOT clicked in KRED)\n",
        "    newPosNegListCol = oldPosList + ' ' + oldNegList                                                                            # build one final list of positive and negative elements as required by KRED in input\n",
        "    newDf.at[index, 'newPositiveList'] = newPosListCol                                                                          # store obtained results in the new columns\n",
        "    newDf.at[index, 'newPosNegList'] = newPosNegListCol\n",
        "\n",
        "  columnsToKeep = ['userId', 'timestamp', 'newPositiveList', 'newPosNegList']                                                   # project on the needed columns only\n",
        "  newDf = newDf[columnsToKeep]\n",
        "  newDf = newDf.rename(columns={'newPositiveList': 'userHistory', 'newPosNegList': 'positiveAndNegativeLists'})                 # give meaningful names\n",
        "\n",
        "  counter += 1\n",
        "  print(f\"\\n\\nCompleted processing of day {counter} behaviors\")\n",
        "  print(newDf.info())\n",
        "\n",
        "\n",
        "  reshapedDfs.append(newDf)                                                                                                     # store the final version of the daily behavior DF\n",
        "\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scs0kCOS8iXk",
        "outputId": "6ecac6eb-429a-471b-da7d-f12ec2eefefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 703985 entries, 0 to 98530\n",
            "Data columns (total 5 columns):\n",
            " #   Column                    Non-Null Count   Dtype \n",
            "---  ------                    --------------   ----- \n",
            " 0   index                     703985 non-null  int64 \n",
            " 1   userId                    703985 non-null  object\n",
            " 2   timestamp                 703985 non-null  int64 \n",
            " 3   userHistory               703985 non-null  object\n",
            " 4   positiveAndNegativeLists  703985 non-null  object\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 32.2+ MB\n",
            "None\n",
            "       index                                       userId   timestamp  \\\n",
            "0          1            cx:hwwz74npvfpbeb6t:3r8a34vl30pzx  1483258706   \n",
            "1          2   cx:35qzmnop24aabdict6ce06jeu:3oh8amzr1ymie  1483235637   \n",
            "2          3   cx:g5w8dqvwmphi3312fz77s1cnu:1lphshweqrtpp  1483268001   \n",
            "3          4   cx:2wdlftbi226e93j6ubrrr9ywlk:xi3bokxgktuc  1483231519   \n",
            "4          5   cx:1tsqelygc061ozuetu9i0pkny:2ko62cresl3dx  1483278506   \n",
            "...      ...                                          ...         ...   \n",
            "98526  98527  cx:2k8lgs1vgl9xz1a6kr7rjx7kjy:2n79xb1k1lwjf  1483743465   \n",
            "98527  98528   cx:3v8gihnk10reauqe9k0y0txe9:2ka2lazfrqc0q  1483743471   \n",
            "98528  98529  cx:18c0bo93pwom21dssre7v527ba:2h8mqy93ylyei  1483743473   \n",
            "98529  98530             cx:hvl65l18rxqbg0p9:999mgd3b8ile  1483743504   \n",
            "98530  98531             cx:i6vg2muycor2g7tb:wqowc9878gun  1483743516   \n",
            "\n",
            "                                             userHistory  \\\n",
            "0      http://adressa.no/nyheter/trondheim/2016/12/29...   \n",
            "1                                      http://adressa.no   \n",
            "2      http://adressa.no http://adressa.no/folk/doeds...   \n",
            "3      http://adressa.no http://adressa.no/nyheter/so...   \n",
            "4                                      http://adressa.no   \n",
            "...                                                  ...   \n",
            "98526  http://adressa.no/nyheter/sortrondelag/2017/01...   \n",
            "98527  http://adressa.no/nyheter/sortrondelag/2017/01...   \n",
            "98528  http://adressa.no http://adressa.no/nyheter/so...   \n",
            "98529  http://adressa.no/nyheter/okonomi/2017/01/06/f...   \n",
            "98530  http://adressa.no/100sport/vintersport/dette-e...   \n",
            "\n",
            "                                positiveAndNegativeLists  \n",
            "0      http://adressa.no/nyheter/trondheim/2016/12/29...  \n",
            "1      http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "2      http://adressa.no-1 http://adressa.no/folk/doe...  \n",
            "3      http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "4      http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "...                                                  ...  \n",
            "98526  http://adressa.no/nyheter/sortrondelag/2017/01...  \n",
            "98527  http://adressa.no/nyheter/sortrondelag/2017/01...  \n",
            "98528  http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "98529  http://adressa.no/nyheter/okonomi/2017/01/06/f...  \n",
            "98530  http://adressa.no/100sport/vintersport/dette-e...  \n",
            "\n",
            "[703985 rows x 5 columns]\n",
            "Saving final dataset as: /content/KRED-Reccomendation-System/data/AdressaProcessed/behavior-week.tsv\n",
            "Saving backup on mounted Drive: /content/drive/MyDrive/AdressaProcessed/behavior-week.tsv\n",
            "\n",
            "Stage completed in 22.89 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "t1 = time()\n",
        "# CONCATENATE DAYS AND SAVE\n",
        "behaviorWeek = pd.concat(reshapedDfs)                                                                       # build weekly behavior DF by concatenation\n",
        "behaviorWeek['index'] = behaviorWeek.index + 1                                                              # build the index column (needed to match KRED input schema)\n",
        "behaviorWeek = behaviorWeek[['index', 'userId', 'timestamp', 'userHistory', 'positiveAndNegativeLists']]\n",
        "\n",
        "outputFileName = \"behavior-week.tsv\"                                                                        # Save weekly df on disk\n",
        "outputPath = \"/content/KRED-Reccomendation-System/data/AdressaProcessed/\" + outputFileName\n",
        "print(f'Saving final dataset as: {outputPath}')\n",
        "if not os.path.exists(\"/content/KRED-Reccomendation-System/data/AdressaProcessed/\"):                        # create directory to store data if doesnt exists\n",
        "  os.makedirs(\"/content/KRED-Reccomendation-System/data/AdressaProcessed/\")\n",
        "\n",
        "behaviorWeek.to_csv(outputPath, sep=\"\\t\", index = False, header = False)                                    # save as .tsv (tab separated values)\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "print(f'Saving backup on mounted Drive: /content/drive/MyDrive/AdressaProcessed/behavior-week.tsv')         # save backup on Google Drive\n",
        "shutil.copy(\"/content/KRED-Reccomendation-System/data/AdressaProcessed/behavior-week.tsv\", \"/content/drive/MyDrive/AdressaProcessed/behavior-week.tsv\")\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nStage completed in {str(round(t2-t1, 2))} seconds\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azhrA-r9I6Sb",
        "outputId": "6aff507c-f0f5-44ae-c848-4fa42b092ecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            0                                            1           2  \\\n",
            "0           1            cx:hwwz74npvfpbeb6t:3r8a34vl30pzx  1483258706   \n",
            "1           2   cx:35qzmnop24aabdict6ce06jeu:3oh8amzr1ymie  1483235637   \n",
            "2           3   cx:g5w8dqvwmphi3312fz77s1cnu:1lphshweqrtpp  1483268001   \n",
            "3           4   cx:2wdlftbi226e93j6ubrrr9ywlk:xi3bokxgktuc  1483231519   \n",
            "4           5   cx:1tsqelygc061ozuetu9i0pkny:2ko62cresl3dx  1483278506   \n",
            "...       ...                                          ...         ...   \n",
            "703980  98527  cx:2k8lgs1vgl9xz1a6kr7rjx7kjy:2n79xb1k1lwjf  1483743465   \n",
            "703981  98528   cx:3v8gihnk10reauqe9k0y0txe9:2ka2lazfrqc0q  1483743471   \n",
            "703982  98529  cx:18c0bo93pwom21dssre7v527ba:2h8mqy93ylyei  1483743473   \n",
            "703983  98530             cx:hvl65l18rxqbg0p9:999mgd3b8ile  1483743504   \n",
            "703984  98531             cx:i6vg2muycor2g7tb:wqowc9878gun  1483743516   \n",
            "\n",
            "                                                        3  \\\n",
            "0       http://adressa.no/nyheter/trondheim/2016/12/29...   \n",
            "1                                       http://adressa.no   \n",
            "2       http://adressa.no http://adressa.no/folk/doeds...   \n",
            "3       http://adressa.no http://adressa.no/nyheter/so...   \n",
            "4                                       http://adressa.no   \n",
            "...                                                   ...   \n",
            "703980  http://adressa.no/nyheter/sortrondelag/2017/01...   \n",
            "703981  http://adressa.no/nyheter/sortrondelag/2017/01...   \n",
            "703982  http://adressa.no http://adressa.no/nyheter/so...   \n",
            "703983  http://adressa.no/nyheter/okonomi/2017/01/06/f...   \n",
            "703984  http://adressa.no/100sport/vintersport/dette-e...   \n",
            "\n",
            "                                                        4  \n",
            "0       http://adressa.no/nyheter/trondheim/2016/12/29...  \n",
            "1       http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "2       http://adressa.no-1 http://adressa.no/folk/doe...  \n",
            "3       http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "4       http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "...                                                   ...  \n",
            "703980  http://adressa.no/nyheter/sortrondelag/2017/01...  \n",
            "703981  http://adressa.no/nyheter/sortrondelag/2017/01...  \n",
            "703982  http://adressa.no-1 http://adressa.no/nyheter/...  \n",
            "703983  http://adressa.no/nyheter/okonomi/2017/01/06/f...  \n",
            "703984  http://adressa.no/100sport/vintersport/dette-e...  \n",
            "\n",
            "[703985 rows x 5 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 703985 entries, 0 to 703984\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   0       703985 non-null  int64 \n",
            " 1   1       703985 non-null  object\n",
            " 2   2       703985 non-null  int64 \n",
            " 3   3       703985 non-null  object\n",
            " 4   4       703985 non-null  object\n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 26.9+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# CHECKING FINAL RESULT\n",
        "inputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'\n",
        "inputPath = inputFolder + 'behavior-week.tsv'\n",
        "df = pd.read_csv(inputPath, header = None, sep='\\t')\n",
        "print(df)\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMnvXbO84VkI"
      },
      "source": [
        "# 1.3.3 Work towards news.tsv format\n",
        "*Now that all news have been extracted one can skip this section and go to 1.3.4 to directly build knowledge graph *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWFlJooPmzTO"
      },
      "outputs": [],
      "source": [
        "# Fill category and add subcategory column starting from url\n",
        "def cat_subcat_from_url(url):\n",
        "  \"\"\"Given an URL as input, return title, category, subcategory as strings\"\"\"\n",
        "  #Parse the URL\n",
        "  parsed_url = urlparse(url)\n",
        "\n",
        "  #Get path from parsed URL\n",
        "  path = parsed_url.path\n",
        "\n",
        "  #Split path into segments divided by '/'\n",
        "  segments = path.split('/')\n",
        "\n",
        "  #Extract desired parts\n",
        "\n",
        "  #Category\n",
        "  category = segments[1]\n",
        "\n",
        "  #Subcategory\n",
        "  if (len(segments) > 2):\n",
        "    subcategory = segments[2]\n",
        "  else:\n",
        "    subcategory = None\n",
        "\n",
        "  return category, subcategory\n",
        "\n",
        "# get wikiid for each entity\n",
        "# Function to retrieve Wikidata ID for an item\n",
        "def get_wikidata_id(item):\n",
        "    # Query Wikidata for the item using its label\n",
        "    url = f'https://www.wikidata.org/w/api.php?action=wbsearchentities&format=json&language=en&search={item}'\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        if 'search' in data:\n",
        "            search_results = data['search']\n",
        "            if search_results:\n",
        "                # Get the first result (highest score)\n",
        "                result = search_results[0]\n",
        "                if 'id' in result:\n",
        "                    return result['id']\n",
        "    return None\n",
        "\n",
        "# Transform 'profile' column\n",
        "def df_get_wikidata_id(df):\n",
        "  df['profile'] = df['profile'].apply(lambda x: [{'Label': item['item'], 'Type': item['group'], 'Confidence': item['weight'], 'WikidataId': get_wikidata_id(item['item'])} for item in x])\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "id": "EwoW9WhddU3R",
        "outputId": "67a2fe8e-a2a1-41eb-c864-ffa34bd92844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Datasets contains the following null values in the fields:\n",
            "eventId               0\n",
            "time                  0\n",
            "userId                0\n",
            "url                   0\n",
            "profile         6941278\n",
            "category        7756434\n",
            "keywords        7391531\n",
            "title           6941278\n",
            "activeTime      4780659\n",
            "sessionStart          0\n",
            "sessionStop           0\n",
            "dtype: int64\n",
            "\n",
            "New df containing only rows with profile:\n",
            " url              0\n",
            "profile          0\n",
            "category    815156\n",
            "keywords    450253\n",
            "title            0\n",
            "dtype: int64\n",
            "\n",
            "Starting to extract info from news URL and update rows...\n",
            "\n",
            "\n",
            "Extraction completed in 989.98 seconds\n",
            "\n",
            "Resulting DataFrame with unique URLs has 20429 rows \n",
            "\n",
            "Starting to modify profile column towards new.tsv format...\n",
            "\n",
            "profile column is already a python dictionary\n",
            "\n",
            "Operation completed in 16.99 seconds\n",
            "\n",
            "Adding WikidataID to news entities and remove etities not found on wikidata server...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "20429it [00:03, 6361.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Operation completed in 3.36 seconds\n",
            "\n",
            "\n",
            "Saving final dataset as: /content/KRED-Reccomendation-System/data/AdressaProcessed/week_updated_news_dataset.tsv\n",
            "Copying final dataset into mounted Drive at: /content/drive/MyDrive/AdressaProcessed/week_updated_news_dataset.tsv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AdressaProcessed/week_updated_news_dataset.tsv'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Observe number of missing values per attribute\n",
        "print(f'The Datasets contains the following null values in the fields:\\n{inputDf.isnull().sum()}')\n",
        "#create dataset with only profile rows\n",
        "df_profile = inputDf.dropna(subset=['profile'])\n",
        "df_profile = df_profile.drop(['eventId', 'time', 'userId', 'activeTime', 'sessionStart', 'sessionStop'], axis=1)\n",
        "print(f'\\nNew df containing only rows with profile:\\n {df_profile.isnull().sum()}')\n",
        "\n",
        "t1 = time()\n",
        "print('\\nStarting to extract info from news URL and update rows...\\n')\n",
        "\n",
        "# Apply the function to the 'url' column and store the results in new columns\n",
        "df_profile[['new_category', 'subcategory']] = df_profile['url'].apply(lambda url: pd.Series(cat_subcat_from_url(url)))\n",
        "\n",
        "# Fill NaN values in the 'category' column using values from 'title' column\n",
        "df_profile['category'].fillna(df_profile['new_category'], inplace=True)\n",
        "df_profile = df_profile.drop(['category'], axis=1)\n",
        "df_profile.rename(columns={'new_category': 'category'}, inplace=True)\n",
        "df_profile.head()# Apply the function to the 'url' column and store the results in new columns\n",
        "df_profile[['new_category', 'subcategory']] = df_profile['url'].apply(lambda url: pd.Series(cat_subcat_from_url(url)))\n",
        "\n",
        "# Fill NaN values in the 'category' column using values from 'title' column\n",
        "df_profile['category'].fillna(df_profile['new_category'], inplace=True)\n",
        "df_profile = df_profile.drop(['category'], axis=1)\n",
        "df_profile.rename(columns={'new_category': 'category'}, inplace=True)\n",
        "df_profile.head()\n",
        "\n",
        "# Remove duplicate rows\n",
        "df_profile = df_profile.drop_duplicates(subset='url', keep='first')\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nExtraction completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "\n",
        "# Print the resulting DataFrame with unique URLs\n",
        "print(f'Resulting DataFrame with unique URLs has {len(df_profile)} rows \\n')\n",
        "\n",
        "t1 = time()\n",
        "print('Starting to modify profile column towards new.tsv format...\\n')\n",
        "\n",
        "\n",
        "# Update the 'profile' column to contain the desired list of dictionaries, remove items with 'group' : ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']\n",
        "#df_profile['profile'].to_dict()\n",
        "try:\n",
        "  df_profile['profile'] = df_profile['profile'].apply(ast.literal_eval)\n",
        "except:\n",
        "  print('profile column is already a python dictionary')\n",
        "df_profile['profile'] = df_profile['profile'].apply(lambda x: [{'item': item['item'], **group} for item in x for group in item['groups']if item['item'] != '0' and group['group'] not in ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']])# Update the 'profile' column to contain the desired list of dictionaries, remove items with 'group' : ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']\n",
        "#df_profile['profile'].to_dict()\n",
        "try:\n",
        "  df_profile['profile'] = df_profile['profile'].apply(ast.literal_eval)\n",
        "  df_profile['profile'] = df_profile['profile'].apply(lambda x: [{'item': item['item'], **group} for item in x for group in item['groups']if item['item'] != '0' and group['group'] not in ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']])\n",
        "except:\n",
        "  print('profile column is already a python dictionary')\n",
        "  # Update the 'profile' column to contain the desired list of dictionaries\n",
        "  df_profile['profile'] = df_profile['profile'].apply(lambda x: [item for item in x if item['item'] != '0' and item['group'] not in ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']])\n",
        "#df_profile['profile'] = df_profile['profile'].apply(lambda x: [{'item': item['item'], **group} for item in x for group in item['groups']if item['item'] != '0' and group['group'] not in ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']])\n",
        "\n",
        "# Update the 'profile' column to contain the desired list of dictionaries\n",
        "#df_profile['profile'] = df_profile['profile'].apply(lambda x: [item for item in x if item['item'] != '0' and item['group'] not in ['site', 'author', 'language', 'pageclass', 'adressa-access', 'adressa-tag']])\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nOperation completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "t1 = time()\n",
        "print('Adding WikidataID to news entities and remove etities not found on wikidata server...\\n')\n",
        "# Load the datasets\n",
        "inputPath_news = inputFolder + SELECTED_INPUT\n",
        "inputPath_ent =  inputFolder + 'entities-week.csv'\n",
        "df_entities = pd.read_csv(inputPath_ent, index_col=False)\n",
        "df_news = df_profile\n",
        "\n",
        "# Create a dictionary to map wikiid values to entities\n",
        "wikiid_dict = dict(zip(df_entities['item'], df_entities['wikiid']))\n",
        "\n",
        "# Iterate over each row in the news dataset\n",
        "for index, row in tqdm(df_news.iterrows()):\n",
        "    profile = row['profile']\n",
        "    updated_profile = []\n",
        "\n",
        "    # Iterate over each dictionary in the 'profile' list\n",
        "    for dictionary in profile:\n",
        "        label = dictionary['item']\n",
        "        group = dictionary['group']\n",
        "        weight = dictionary['weight']\n",
        "        count = dictionary['count']\n",
        "\n",
        "        # Get the corresponding wikiid from the entities dataset\n",
        "        wikiid = wikiid_dict.get(label)\n",
        "\n",
        "        # Only add dictionaries with a valid wikiid\n",
        "        if wikiid is not None:\n",
        "            updated_dict = {\n",
        "                'Label': label,\n",
        "                'Type': group,\n",
        "                'WikidataId': wikiid,\n",
        "                'Confidence': weight,\n",
        "                'OccurrenceOffsets': count,\n",
        "                'SurfaceForms': [label.split(',')[0]]  # Modify?\n",
        "            }\n",
        "            updated_profile.append(updated_dict)\n",
        "\n",
        "    # Update the 'profile' column with the updated profile list\n",
        "    df_news.at[index, 'profile'] = updated_profile\n",
        "\n",
        "t2 = time()\n",
        "print(f\"\\nOperation completed in {str(round(t2-t1, 2))} seconds\\n\")\n",
        "\n",
        "# Update the output path construction\n",
        "outputFileName = SELECTED_INPUT.split('.')[0] + '_updated_news_dataset.tsv'\n",
        "outputPath = outputFolder + outputFileName\n",
        "\n",
        "print(f'\\nSaving final dataset as: {outputPath}')\n",
        "\n",
        "# Save the updated dataset as TSV\n",
        "df_news.to_csv(outputPath, sep='\\t', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
        "\n",
        "# Copy the file to the mounted drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\" + outputFileName\n",
        "print(f'Copying final dataset into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf1HefSTDKXO",
        "outputId": "c1a67a43-6e9d-4d51-dbcd-89bcde0eb3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "new dataset length 1094 and it has url               0\n",
            "profile           0\n",
            "keywords       1092\n",
            "title             0\n",
            "category          0\n",
            "subcategory       2\n",
            "dtype: int64 empty rows \n",
            "\n"
          ]
        }
      ],
      "source": [
        "news_path = '/content/drive/MyDrive/AdressaProcessed/news_day2.csv'\n",
        "if SELECTED_INPUT != 'day2.csv':\n",
        "  # First dataset\n",
        "  df1 = pd.read_csv(news_path, index_col=False, header= None)\n",
        "  df1.columns = ['url']\n",
        "\n",
        "  # Second dataset\n",
        "  df2 = df_profile\n",
        "\n",
        "  # Merge the datasets based on 'url' column\n",
        "  merged = df2.merge(df1, on='url', how='left')\n",
        "\n",
        "  # Filter the rows where the other columns are not null (not present in the first dataset)\n",
        "  filtered_df2 = merged[merged['url'].isnull() | merged['profile'].isnull() | merged['keywords'].isnull() | merged['title'].isnull() | merged['category'].isnull() | merged['subcategory'].isnull()]\n",
        "  df_profile = filtered_df2\n",
        "  # Print the filtered dataframe\n",
        "  print(f'new dataset length {len(filtered_df2)} and it has {filtered_df2.isnull().sum()} empty rows \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlwI2RyxxPHk",
        "outputId": "30001ee8-10d1-4008-c7ba-5efc35a3872c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1094/1094 [00:02<00:00, 506.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " The resulting list of entities contains 12406 items\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty list\n",
        "item_list = []\n",
        "\n",
        "# Iterate over each row in the column\n",
        "for row in tqdm(df_profile['profile']):\n",
        "    # Iterate over each dictionary in the row\n",
        "    for dictionary in row:\n",
        "        # Extract the 'item' value from the dictionary\n",
        "        item = dictionary['item']\n",
        "\n",
        "        # Check if the 'item' value is already in the list\n",
        "        if item not in item_list:\n",
        "            # Append the 'item' value to the list\n",
        "            item_list.append(item)\n",
        "\n",
        "# Print the resulting list length\n",
        "print(f'\\n The resulting list of entities contains {len(item_list)} items')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qurgbIKqyhiH",
        "outputId": "c71da11c-25a5-43b0-b729-84b5d1bbed5a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12406/12406 [26:35<00:00,  7.78it/s]\n"
          ]
        }
      ],
      "source": [
        "# Create a list of dictionaries with 'item' and 'wikiid'\n",
        "list_ent = []\n",
        "\n",
        "# Iterate over the item_list\n",
        "for el in tqdm(item_list):\n",
        "    # Create a new dictionary for each item\n",
        "    dict_ent = {}\n",
        "    dict_ent['item'] = el\n",
        "    dict_ent['wikiid'] = get_wikidata_id(el)\n",
        "\n",
        "    # Append the dictionary to the list\n",
        "    list_ent.append(dict_ent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Fl7gl0Eh0xLN",
        "outputId": "bb2f46b5-65a1-4d06-9c60-24475d4fd756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saving final dataset as: /content/KRED-Reccomendation-System/data/AdressaProcessed/entities-day7.csv\n",
            "Copying final dataset into mounted Drive at: /content/drive/MyDrive/AdressaProcessed/entities-day7.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AdressaProcessed/entities-day7.csv'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# save current day entities and wiki_id into a csv\n",
        "#DELETE = False                                                # cleanup dataframe at the end?\n",
        "\n",
        "outputFileName = SELECTED_INPUT                               # usually \"day1.csv\" or \"week.csv\"\n",
        "outputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'\n",
        "outputPath = outputFolder + 'entities-' + outputFileName      # '/content/KRED-Reccomendation-System/data/AdressaProcessed/entities-day1.csv'\n",
        "print(f'\\nSaving final dataset as: {outputPath}')\n",
        "df_entites = pd.DataFrame.from_dict(list_ent)\n",
        "df_entites.to_csv(outputPath, index = True)\n",
        "\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\"+'entities-' + outputFileName\n",
        "print(f'Copying final dataset into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "O6iV8Q0gQklQ",
        "outputId": "138e182c-fac9-4b54-ac41-68510d5672bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copying final dataset into mounted Drive at: /content/drive/MyDrive/AdressaProcessed/news_news_day2.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AdressaProcessed/news_news_day2.csv'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if SELECTED_INPUT != 'day2.csv':\n",
        "  # Append the DataFrame rows to the existing file\n",
        "  outputPath = 'news_day2.csv'\n",
        "  df_profile['url'].to_csv(outputPath, header=False, index=False, mode='a')\n",
        "else:\n",
        "  outputPath = 'news_'+ outputFileName +'.txt'\n",
        "  file = open('news_'+ outputFileName +'.txt','w')\n",
        "  for item in df_profile['url']:\n",
        "    file.write(item+\"\\n\")\n",
        "  file.close()\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\"+'news_' + outputPath\n",
        "print(f'Copying final dataset into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B06Vsgamoha"
      },
      "source": [
        "## 1.3.4 Build KG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMIH-Qdond6m"
      },
      "outputs": [],
      "source": [
        "def get_entity_info(entity_id):\n",
        "    response = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\")\n",
        "    if response.status_code == 200:\n",
        "        entity_data = response.json()\n",
        "        entities = entity_data.get('entities', {})\n",
        "        if entity_id in entities:\n",
        "            return entities[entity_id]\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "OtPR2WaCIRVP",
        "outputId": "6b258b10-5b19-4d40-fd71-beb40d1d5f7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we have collected 71394 entities\n",
            "\n",
            "Saving final dataset as: /content/KRED-Reccomendation-System/data/AdressaProcessed/entities-week.csv\n",
            "Copying final dataset into mounted Drive at: /content/drive/MyDrive/AdressaProcessed/entities-week.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AdressaProcessed/entities-week.csv'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_path = \"/content/drive/MyDrive/AdressaProcessed/\"\n",
        "# Concatenate datasets vertically\n",
        "# First dataset\n",
        "df1 = pd.read_csv(input_path + 'entities-day1.csv', index_col=False) # entities extracted from day 1\n",
        "df1.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df2 = pd.read_csv(input_path + 'entities-day2.csv', index_col=False) # entities extracted from day 2\n",
        "df2.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df3 = pd.read_csv(input_path + 'entities-day3.csv', index_col=False) # entities extracted from day 3\n",
        "df3.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df4 = pd.read_csv(input_path + 'entities-day4.csv', index_col=False) # entities extracted from day 4\n",
        "df4.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df5 = pd.read_csv(input_path + 'entities-day5.csv', index_col=False) # entities extracted from day 5\n",
        "df5.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df6 = pd.read_csv(input_path + 'entities-day6.csv', index_col=False) # entities extracted from day 6\n",
        "df6.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "df7 = pd.read_csv(input_path + 'entities-day7.csv', index_col=False) # entities extracted from day 7\n",
        "df7.dropna(subset=['wikiid'], inplace=True) # Drop rows with NaN values in the 'wikiid' column\n",
        "\n",
        "# The ignore_index=True option resets the index of the appended dataset\n",
        "appended_df = pd.concat([df1, df2, df3, df4, df5, df6, df7], ignore_index=True)\n",
        "\n",
        "# Remove duplicates from the DataFrame\n",
        "appended_df = appended_df.drop_duplicates()\n",
        "# Drop unamed column\n",
        "appended_df = appended_df.drop('Unnamed: 0', axis=1)\n",
        "# Print the appended dataset length\n",
        "print(f'we have collected {len(appended_df)} entities')\n",
        "\n",
        "outputFileName = 'week.csv'                            # usually \"day1.csv\" or \"week.csv\"\n",
        "outputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'\n",
        "outputPath = outputFolder + 'entities-' + outputFileName      # '/content/KRED-Reccomendation-System/data/AdressaProcessed/entities-day1.csv'\n",
        "print(f'\\nSaving final dataset as: {outputPath}')\n",
        "appended_df.to_csv(outputPath, index = True)\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\"+'entities-' + outputFileName\n",
        "print(f'Copying final dataset into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-56SGmgwgUtc",
        "outputId": "9d9a48bb-4528-45fd-9f80-29d9fd547d48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71394/71394 [2:28:56<00:00,  7.99it/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33681/33681 [00:02<00:00, 11567.58it/s]\n"
          ]
        }
      ],
      "source": [
        "def get_entity_info(entity_id):\n",
        "    response = requests.get(f\"https://www.wikidata.org/wiki/Special:EntityData/{entity_id}.json\")\n",
        "    if response.status_code == 200:\n",
        "        entity_data = response.json()\n",
        "        entities = entity_data.get('entities', {})\n",
        "        if entity_id in entities:\n",
        "            return entities[entity_id]\n",
        "    return None\n",
        "\n",
        "\n",
        "entity_ids = appended_df['wikiid']  # Entity IDs to build to kg\n",
        "entities = {}\n",
        "for entity_id in tqdm(entity_ids):\n",
        "    entity_info = get_entity_info(entity_id)\n",
        "    if entity_info:\n",
        "        entities[entity_id] = entity_info\n",
        "\n",
        "knowledge_graph = {}\n",
        "for entity_id, entity_info in tqdm(entities.items()):\n",
        "    knowledge_graph[entity_id] = {}\n",
        "    if 'claims' in entity_info:\n",
        "        claims = entity_info['claims']\n",
        "        for prop, values in claims.items():\n",
        "            property_values = []\n",
        "            for value in values:\n",
        "                if 'mainsnak' in value and 'datavalue' in value['mainsnak']:\n",
        "                    data_value = value['mainsnak']['datavalue']\n",
        "                    if data_value.get('type') == 'wikibase-entityid' and 'value' in data_value:\n",
        "                        property_values.append(data_value['value']['id'])\n",
        "            if property_values:\n",
        "                knowledge_graph[entity_id][prop] = property_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "pgnIQlaXkGSb",
        "outputId": "f337fce8-2d7a-4314-ce55-0b89ded8b1a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Knowledge graph saved to 'knowledge_graph_addressa.tsv'.\n",
            "\n",
            " Copying final KG into mounted Drive at: /content/drive/MyDrive/AdressaProcessed/knowledge_graph_addressa.tsv\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/AdressaProcessed/knowledge_graph_addressa.tsv'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Specify the output file name\n",
        "output_file = 'knowledge_graph_addressa.tsv'\n",
        "outputFolder = '/content/KRED-Reccomendation-System/data/AdressaProcessed/'\n",
        "outputPath = output_file     # '/content/KRED-Reccomendation-System/data/AdressaProcessed/knowledge_graph_addressa.tsv'\n",
        "\n",
        "# Write the knowledge graph to the .tsv file\n",
        "with open(output_file, 'w', newline='') as f:\n",
        "    writer = csv.writer(f, delimiter='\\t')\n",
        "\n",
        "    # Iterate over each entity and its properties in the knowledge graph\n",
        "    for entity_id, properties in knowledge_graph.items():\n",
        "        for prop, values in properties.items():\n",
        "            # Write each relationship to a row in the .tsv file\n",
        "            for value in values:\n",
        "                writer.writerow([entity_id, prop, value])\n",
        "\n",
        "print(f\"Knowledge graph saved to '{outputFolder + output_file}'.\")\n",
        "\n",
        "# save on drive --> i.e. copy files in drive folder\n",
        "mountedDriveOutputPath = \"/content/drive/MyDrive/AdressaProcessed/\"+ output_file\n",
        "print(f'\\n Copying final KG into mounted Drive at: {mountedDriveOutputPath}')\n",
        "shutil.copy(outputPath, mountedDriveOutputPath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPwdSBGYQLdO"
      },
      "source": [
        "## 1.3.5 Execution of the Adressa extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ITbyx01QTSt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('')\n",
        "import os\n",
        "\n",
        "import argparse\n",
        "from parse_config import ConfigParser\n",
        "from utils.util import *\n",
        "from train_test import *\n",
        "\n",
        "data_path = \"/content/KRED-Reccomendation-System/data/AdressaProcessed\"\n",
        "\n",
        "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
        "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
        "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
        "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
        "knowledge_graph_file = os.path.join(data_path, 'kg/kg', r'wikidata-graph.tsv')\n",
        "entity_embedding_file = os.path.join(data_path, 'kg/kg', r'entity2vecd100.vec')\n",
        "relation_embedding_file = os.path.join(data_path, 'kg/kg', r'relation2vecd100.vec')\n",
        "\n",
        "# mind_url, mind_train_dataset, mind_dev_dataset, _ = get_mind_data_set(MIND_type)\n",
        "\n",
        "#kg_url = \"https://kredkg.blob.core.windows.net/wikidatakg/\"\n",
        "\n",
        "#if not os.path.exists(train_news_file):\n",
        "#    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
        "\n",
        "#if not os.path.exists(valid_news_file):\n",
        "#    download_deeprec_resources(mind_url, \\\n",
        "#                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
        "\n",
        "#if not os.path.exists(knowledge_graph_file):\n",
        "#    download_deeprec_resources(kg_url, \\\n",
        "#                               os.path.join(data_path, 'kg'), \"kg\")\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='KRED')\n",
        "\n",
        "\n",
        "parser.add_argument('-c', '--config', default=\"./config.yaml\", type=str,\n",
        "                    help='config file path (default: None)')\n",
        "parser.add_argument('-r', '--resume', default=None, type=str,\n",
        "                    help='path to latest checkpoint (default: None)')\n",
        "parser.add_argument('-d', '--device', default=None, type=str,\n",
        "                    help='indices of GPUs to enable (default: all)')\n",
        "\n",
        "config = ConfigParser.from_args(parser)\n",
        "# print(config['data'])\n",
        "\n",
        "def limit_user2item_validation_data(data, size):\n",
        "    test_data = data[-1]\n",
        "    test_data_reduced = {key: test_data[key][:size] for key in test_data.keys()}\n",
        "    # Concatenate the old tuple with the updated validation data\n",
        "    return data[:-1] + (test_data_reduced,)\n",
        "\n",
        "\n",
        "epochs = 1\n",
        "batch_size = 64\n",
        "train_type = \"single_task\"\n",
        "task = \"user2item\" # task should be within: user2item, item2item, vert_classify, pop_predict\n",
        "\n",
        "config['trainer']['epochs'] = epochs\n",
        "config['data_loader']['batch_size'] = batch_size\n",
        "config['trainer']['training_type'] = train_type\n",
        "config['trainer']['task'] = task\n",
        "config['trainer']['save_period'] = epochs/2\n",
        "# The following parameters define which of the extensions are used,\n",
        "# by setting them to False the original KRED model is executed\n",
        "#if not os.path.isfile(f\"{config['data']['sentence_embedding_folder']}/train_news_embeddings.pkl\"):\n",
        "#  write_embedding_news(\"./data/train\", config[\"data\"][\"sentence_embedding_folder\"])\n",
        "\n",
        "#if not os.path.isfile(f\"{config['data']['sentence_embedding_folder']}/valid_news_embeddings.pkl\"):\n",
        "#  write_embedding_news(\"./data/valid\", config[\"data\"][\"sentence_embedding_folder\"])\n",
        "\n",
        "data = load_data_mind(config, config['data']['sentence_embedding_folder'])\n",
        "print(\"Data loaded, ready for training\")\n",
        "#if not os.path.isfile(f\"{data_path}/data_mind.pkl\"):\n",
        "#    write_data_mind(config, data_path)\n",
        "#data = read_pickle(f\"{data_path}/data_mind.pkl\")\n",
        "\n",
        "#test_data = data[-1]\n",
        "#data = limit_user2item_validation_data(data, 10000)\n",
        "print(\"Data loaded, ready for training\")\n",
        "#single_task_training(config, data)  # user2item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o9wzI-cTGEy",
        "outputId": "a0ab2706-103d-4259-fed8-a88b25a0c6a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jul 14 21:34:34 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byDEHHvSVIl9",
        "outputId": "f5561a6b-4f75-4a1d-c616-8fe359f41ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8XLZ6LrVaNb",
        "outputId": "f6b9437f-42d7-4a02-9ef9-6a698d622c34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.0.1+cu118)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchvision) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade torch\n",
        "!pip install --upgrade torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TRZixBSdqU0z"
      },
      "outputs": [],
      "source": [
        "t1 = time()\n",
        "!python addressa_rec_training.py\n",
        "t2 = time()\n",
        "print(f\"\\n\\nStage completed in {str(round((t2-t1)/60, 2))} minutes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWVSUse7YSbK"
      },
      "source": [
        "# **Extension 2: MindReader dataset for movies recommendation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lj3grpGfyhM"
      },
      "outputs": [],
      "source": [
        "!python data_preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PkY01x9HWP9",
        "outputId": "cb8f4292-957c-4d93-9215-83a2739b3c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constructing embedding ...\n",
            "constructing adjacency matrix ...\n",
            "constructing embedding ...\n"
          ]
        }
      ],
      "source": [
        "!python movies_rec_training.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiIKgOvLCH4Y",
        "outputId": "2e657dff-30bf-4dc6-d68b-15a2a9035ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun May 21 17:02:03 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUb3ndwxCJgC"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/MyDrive/KRED/KRED/data/mind_reader_dataset/movies.pkl\" \"/content/KRED-Reccomendation-System/data/mind_reader_dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Execution of base model"
      ],
      "metadata": {
        "id": "C5dEAV-z_X7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python News_rec_training.py"
      ],
      "metadata": {
        "id": "fynaevCfR19J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}